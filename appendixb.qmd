# Useful Results and Definitions

{{< include _macros.tex >}}

## Probability and Counting

### Multiplication Rule

Consider a series of two trials, where the first trial has $a$ possible outcomes and for each of these outcomes, the second trial has $b$ possible outcomes. Then the total number of possible pairs of outcomes from the two trials is $ab.$

### Naive Definition of Probability

Let $S$ be the sample space where every outcome is equally likely. If $A \subseteq S$ is an event, then

$$
P(A) = \frac{|A|}{|S|},
$$

the number of outcomes in $A$ divided by the number of outcomes in $S.$

### Principle of Inclusion Exclusion (PIE)

If $A$ and $B$ are events, then

$$
P(A \cup B) = P(A) + P(B) - P(A, B).
$$

If $C$ is also an event, then

$$
P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A,B)-P(A,C)-P(B,C)+P(A,B,C).
$$

**Could be good to put Venn diagrams here.**

In general, if $A_1, \dots, A_n$ are events, then

$$
P\left(\bigcup_{i=1}^n A_i\right) = \sum_{i} P(A_i) - \sum_{i < j}P(A_i,A_j) + \sum_{i < j < k} P(A_i,A_j,A_k) - \dots + (-1)^{n+1}P(A_1, \dots, A_n).
$$

In words, we add all of the single probabilities up, then subtract all the pairwise probabilities, then add back all the triplet probabilities, subtract all the quadruple probabilities, etc.

### Binomial Coefficients

If there are $n$ distinct objects and we pick $k$ of them without replacement, then the total number of possible sets of $k$ objects (in other words, order does not matter) we can end up with is given by

$$
{n \choose k} = \frac{n!}{(n-k)!k!}.
$$

### Bose-Einstein

If there are $n$ distinct objects and we choose $k$ times **with** replacement and order does not matter, then the total number of ways this can happen is

$$
{n + k-1 \choose k}.
$$

If each choice is equally likely to be any of the $n$ objects and the choices are independent of one another, then these outcomes are **not** equally likely. Hence, using the naive definition of probability on Bose-Einstein will fail.

## Conditional Probability

### Conditional Probability

If $A$ and $B$ are events, then the probability of $A$ given $B,$ notated as $P(A|B),$ is given by

$$
P(A|B) = \frac{P(A,B)}{P(B)}.
$$

The equation above also gives us this equality:

$$
P(A,B) = P(A|B)P(B) = P(B|A)P(A).
$$

Note that $P(A|B)$ and $P(B|A)$ are different things. Falsely equating these two is known as the **Prosecutor's Fallacy**.

### Bayes' Rule

If $A$ and $B$ are events, then
$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}.
$$
Moreover, if $C$ is an event, then we can get Bayes' Rule with extra conditioning:
$$
P(A|B,C) = \frac{P(B|A,C)P(A|C)}{P(B|C)}.
$$
Note that we add conditioning on $C$, and we condition on $C$ for every single probability in the equation. An easy way to check if you have applied Bayes' Rule with extra conditioning properly is to see if the additioning condition you're adding, in this case $C$, appears after the conditioning bar in every single probability. If there is any probability with no conditioning bar or if there is any probability where $C$ appears *before* the conditioning bar, that is incorrect!

### Law of Total Probability (LOTP)

Suppose $A$ is an event and $B_1, \dots, B_k$ form a partition of the sample space. Then,
$$
P(A) = \sum_{i=1}^k P(A|B_i)P(B_i).
$$
One useful partition is $B, B^c,$ in which case we get
$$
P(A) = P(A|B)P(B)+P(A|B^c)P(B^c).
$$
We can also add extra conditioning on $C$ to get LOTP with extra conditioning:
$$
P(A|C) = \sum_{i=1}^k P(A|B_i,C)P(B_i|C).
$$
Once again, note that $C$ appears after the conditioning bar in *every single* probability. If there is any probability with no conditioning bar or there is a probability with $C$ before the conditioning bar when you are doing LOTP with extra conditioning, that is incorrect!

Combining Bayes' Rule and LOTP, this equation forms, which is very useful:
$$
P(A|B) = \frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|A^c)P(A^c)}.
$$

### Independence

Two events $A$ and $B$ are independent if
$$
P(A,B) = P(A)P(B).
$$
This is equivalent to $P(A|B) = P(A).$ Intuitively, independence means that knowing whether one event happened tells you nothing about the probability of the other event happening.

$A$ and $B$ are conditionally independent given $C$ if
$$
P(A,B|C) = P(A|C)P(B|C).
$$
This means that given $C,$ knowing additionally whether one event happened still tells you nothing about the probability of the other event happening.

Independence and conditional independence are different things and do not imply each other. Conditional independence given $C$ also does not imply conditional independence given $C^c.$ Can you think of examples?

## Random Variables and their Distributions

### Random Variable
Given an experiment with sample space $S,$ a random variable is a function from $S$ to the real numbers. For example, if we are rolling a 6-sided die and $X$ is defined to be two times the side that is rolled, then $X=2$ when a 1 is rolled, $X=4$ when a 2 is rolled, etc. Random variables are often shortened to r.v.s. The support of a random variable is the set of possible values that the r.v. can take on. In our example, the support is $\{2,4,6,8,10,12\}.$

### Probability Mass Functions (PMF)
A Probability Mass Funcction, or PMF, gives the probability of an r.v. taking on each value in its support. That is, for each $x$ in the support of an r.v. $X,$ the PMF of $X$ defines $P(X=x).$ Every valid PMF should sum up to 1 over its support. A PMF determines a random variable's distribution.

### Cumulative Distribution Function (CDF)
A Cumulative Distribution Function, or CDF, of a r.v. $X$ is a function $F_X(x)$ that maps the real numbers to $[0,1].$ It is defined as $F_X(x) = P(X \leq x).$ A CDF must be nondecreasing, right-continuous, and have $\lim_{x \to -\infty}F(x) = 0$ and $\lim_{x \to \infty} F(x) = 1.$ A CDF determines a random variable's distribution.

### Distributions
Notable distributions include the Bernoulli, Binomial, Geometric, Negative Binomial, and Hypergeometric, whose definitions can be found in Appendix A.

### Indicator Random Variables
Given an event $A,$ the random variable $I(A)$ which is equal to 1 when $A$ occurs and equal to 0 when $A$ does not occur is known as an indicator random variable. We have that $I(A) \sim \Bern(P(A))$ since $I(A)$ is equal to 1 with probability $P(A)$ and equal to 0 with probability $1-P(A).$